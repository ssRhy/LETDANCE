#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LETDANCE å§¿æ€åˆ†æå·¥å…·
åŸºäºLangChainçš„å§¿æ€åˆ†æå·¥å…·ï¼Œæ”¯æŒYOLOå§¿æ€æ£€æµ‹å’Œæ‹‰ç­è¿åŠ¨åˆ†æ
"""

import os
import cv2
import json
import math
import logging
import numpy as np
from datetime import datetime
from collections import deque
from typing import Optional, Dict, List, Type, Any

from langchain_core.tools import BaseTool
from langchain_core.callbacks import CallbackManagerForToolRun
from pydantic import BaseModel, Field

try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    logging.warning("YOLOæœªå®‰è£…ï¼Œå§¿æ€æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨")

# å¯¼å…¥å›¾åƒå­˜å‚¨ç®¡ç†å™¨
from .image_storage_utils import storage_manager

# é…ç½®å·²ç›´æ¥å¡«å…¥ï¼Œæ— éœ€å¯¼å…¥config

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å§¿æ€å…³é”®ç‚¹åç§° - ç®€åŒ–ç‰ˆï¼ŒåªåŒ…å«å¿…è¦çš„8ä¸ªå…³é”®ç‚¹
KEYPOINT_NAMES = [
    'å·¦è‚©', 'å³è‚©', 'å·¦è‚˜', 'å³è‚˜',
    'å·¦é«‹', 'å³é«‹', 'å·¦è†', 'å³è†'
]

# YOLOå…³é”®ç‚¹åˆ°ç®€åŒ–å…³é”®ç‚¹çš„ç´¢å¼•æ˜ å°„
YOLO_TO_SIMPLIFIED_MAPPING = {
    0: 5,   # å·¦è‚© -> YOLOç´¢å¼•5
    1: 6,   # å³è‚© -> YOLOç´¢å¼•6
    2: 7,   # å·¦è‚˜ -> YOLOç´¢å¼•7
    3: 8,   # å³è‚˜ -> YOLOç´¢å¼•8
    4: 11,  # å·¦é«‹ -> YOLOç´¢å¼•11
    5: 12,  # å³é«‹ -> YOLOç´¢å¼•12
    6: 13,  # å·¦è† -> YOLOç´¢å¼•13
    7: 14   # å³è† -> YOLOç´¢å¼•14
}

class PoseAnalysisInput(BaseModel):
    """å§¿æ€åˆ†æå·¥å…·è¾“å…¥æ¨¡å‹"""
    action: str = Field(
        description="æ“ä½œç±»å‹ï¼š'analyze_realtime'(å®æ—¶åˆ†æ), 'get_summary'(è·å–æ±‡æ€»)"
    )
    model_path: Optional[str] = Field(
        default="yolov8n-pose.pt",
        description="YOLOæ¨¡å‹è·¯å¾„"
    )
    confidence_threshold: float = Field(
        default=0.5,
        description="æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼"
    )
    duration: Optional[int] = Field(
        default=10,
        description="å®æ—¶åˆ†ææŒç»­æ—¶é—´ï¼ˆç§’ï¼‰"
    )
    save_frames: Optional[bool] = Field(
        default=True,
        description="æ˜¯å¦ä¿å­˜å…³é”®å¸§åˆ°æœ¬åœ°å­˜å‚¨"
    )
    save_interval: Optional[int] = Field(
        default=30,
        description="ä¿å­˜å¸§çš„é—´éš”ï¼ˆæ¯Nå¸§ä¿å­˜ä¸€æ¬¡ï¼‰"
    )

class LabanMovementAnalyzer:
    """åŸºäºæ‹‰ç­è¿åŠ¨åˆ†æç†è®ºçš„æƒ…æ„Ÿè¯†åˆ«å™¨ - ç®€åŒ–ç‰ˆï¼Œåªä½¿ç”¨8ä¸ªå…³é”®ç‚¹"""
    
    def __init__(self, history_length=10):
        self.history_length = history_length
        self.keypoint_history = deque(maxlen=history_length)
        
        # æ‹‰ç­è¿åŠ¨è´¨é‡ç»´åº¦
        self.effort_qualities = {
            'weight': 0,    # é‡é‡ï¼šè½»-é‡
            'time': 0,      # æ—¶é—´ï¼šå¿«-æ…¢
            'flow': 0,      # æµåŠ¨æ€§ï¼šè‡ªç”±-çº¦æŸ
            'space': 0      # ç©ºé—´ï¼šç›´æ¥-é—´æ¥
        }
        
        # æƒ…æ„Ÿæ˜ å°„ï¼ˆé’ˆå¯¹èˆè¹ˆåŠ¨ä½œä¼˜åŒ–ï¼‰
        self.emotion_map = {
            'å¿«ä¹/æ¬¢å¿«': {'weight': 0.5, 'time': 0.6, 'flow': 0.7, 'space': 0.4},
            'ä¼˜é›…/å¹³é™': {'weight': -0.3, 'time': -0.4, 'flow': 0.8, 'space': -0.2},
            'æ¿€æƒ…/å…´å¥‹': {'weight': 0.8, 'time': 0.7, 'flow': 0.5, 'space': 0.6},
            'å¿§éƒ/æ‚²ä¼¤': {'weight': -0.6, 'time': -0.5, 'flow': -0.3, 'space': -0.4},
            'ç´§å¼ /ç„¦è™‘': {'weight': 0.4, 'time': 0.3, 'flow': -0.6, 'space': -0.5},
            'æ”¾æ¾/èˆ’ç¼“': {'weight': -0.4, 'time': -0.6, 'flow': 0.6, 'space': 0.2},
            'åŠ›é‡/å†³å¿ƒ': {'weight': 0.7, 'time': 0.2, 'flow': 0.3, 'space': 0.5},
            'è½»ç›ˆ/é£˜é€¸': {'weight': -0.7, 'time': 0.4, 'flow': 0.8, 'space': 0.3}
        }
    
    def calculate_distance(self, p1, p2):
        """è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»"""
        return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
    
    def calculate_angle(self, p1, p2, p3):
        """è®¡ç®—ä¸‰ç‚¹å½¢æˆçš„è§’åº¦"""
        v1 = (p1[0] - p2[0], p1[1] - p2[1])
        v2 = (p3[0] - p2[0], p3[1] - p2[1])
        
        dot_product = v1[0] * v2[0] + v1[1] * v2[1]
        magnitude1 = math.sqrt(v1[0]**2 + v1[1]**2)
        magnitude2 = math.sqrt(v2[0]**2 + v2[1]**2)
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0
        
        cos_angle = dot_product / (magnitude1 * magnitude2)
        cos_angle = max(-1, min(1, cos_angle))
        
        return math.degrees(math.acos(cos_angle))
    
    def analyze_body_expansion(self, keypoints):
        """åˆ†æèº«ä½“æ‰©å¼ åº¦ - Shapeç»´åº¦ï¼ˆåŸºäº8ä¸ªå…³é”®ç‚¹ï¼‰"""
        if len(keypoints) < 8:
            return 0
        
        # å…³é”®ç‚¹ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š0=å·¦è‚©, 1=å³è‚©, 2=å·¦è‚˜, 3=å³è‚˜, 4=å·¦é«‹, 5=å³é«‹, 6=å·¦è†, 7=å³è†
        
        # è®¡ç®—è‚©å®½ï¼ˆåŸºå‡†ï¼‰
        left_shoulder = keypoints[0][:2]   # å·¦è‚©
        right_shoulder = keypoints[1][:2]  # å³è‚©
        shoulder_width = self.calculate_distance(left_shoulder, right_shoulder)
        
        if shoulder_width <= 0:
            return 0
        
        # è®¡ç®—æ‰‹è‡‚å±•å¼€ç¨‹åº¦ï¼ˆä½¿ç”¨è‚˜éƒ¨ï¼‰
        left_elbow = keypoints[2][:2]   # å·¦è‚˜
        right_elbow = keypoints[3][:2]  # å³è‚˜
        arm_span = self.calculate_distance(left_elbow, right_elbow)
        
        # è®¡ç®—è…¿éƒ¨å¼ å¼€ç¨‹åº¦ï¼ˆä½¿ç”¨è†éƒ¨ï¼‰
        left_knee = keypoints[6][:2]   # å·¦è†
        right_knee = keypoints[7][:2]  # å³è†
        leg_span = self.calculate_distance(left_knee, right_knee)
        
        # ç»¼åˆèº«ä½“æ‰©å¼ åº¦ï¼ˆå½’ä¸€åŒ–åˆ°-1åˆ°1ï¼‰
        arm_expansion = (arm_span / shoulder_width - 1.2) / 1.2  # è‚˜éƒ¨æ­£å¸¸æ¯”ä¾‹çº¦1.2
        leg_expansion = (leg_span / shoulder_width - 0.8) / 0.8  # è†éƒ¨æ­£å¸¸æ¯”ä¾‹çº¦0.8
        
        # é™åˆ¶åœ¨-1åˆ°1ä¹‹é—´
        expansion = (arm_expansion + leg_expansion) / 2
        return max(-1, min(1, expansion))
    
    def analyze_vertical_position(self, keypoints):
        """åˆ†æèº«ä½“å‚ç›´ä½ç½® - Shapeç»´åº¦ï¼ˆåŸºäºèº¯å¹²å…³é”®ç‚¹ï¼‰"""
        if len(keypoints) < 8:
            return 0
        
        # è®¡ç®—èº¯å¹²é‡å¿ƒï¼šè‚©è†€å’Œé«‹éƒ¨çš„ä¸­ç‚¹
        left_shoulder = keypoints[0][:2]   # å·¦è‚©
        right_shoulder = keypoints[1][:2]  # å³è‚©
        left_hip = keypoints[4][:2]        # å·¦é«‹
        right_hip = keypoints[5][:2]       # å³é«‹
        
        shoulder_center_y = (left_shoulder[1] + right_shoulder[1]) / 2
        hip_center_y = (left_hip[1] + right_hip[1]) / 2
        
        # è®¡ç®—èº¯å¹²é•¿åº¦
        torso_length = abs(hip_center_y - shoulder_center_y)
        
        if torso_length <= 0:
            return 0
        
        # è®¡ç®—è†éƒ¨ç›¸å¯¹äºé«‹éƒ¨çš„ä½ç½®
        left_knee = keypoints[6][:2]   # å·¦è†
        right_knee = keypoints[7][:2]  # å³è†
        knee_center_y = (left_knee[1] + right_knee[1]) / 2
        
        # å‚ç›´åç§»ï¼ˆè´Ÿå€¼è¡¨ç¤ºè†éƒ¨è¾ƒé«˜ï¼Œæ­£å€¼è¡¨ç¤ºè†éƒ¨è¾ƒä½ï¼‰
        vertical_offset = (knee_center_y - hip_center_y) / torso_length
        
        # ç¿»è½¬ç¬¦å·ï¼Œæ­£å€¼è¡¨ç¤ºå‘ä¸Šçš„åŠ¨ä½œå€¾å‘
        return -max(-1, min(1, vertical_offset))
    
    def analyze_movement_speed(self, current_keypoints):
        """åˆ†æè¿åŠ¨é€Ÿåº¦ - Timeç»´åº¦ï¼ˆåŸºäº8ä¸ªå…³é”®ç‚¹ï¼‰"""
        if len(self.keypoint_history) < 2:
            return 0
        
        prev_keypoints = self.keypoint_history[-1]
        
        # ä½¿ç”¨æ‰€æœ‰8ä¸ªå…³é”®ç‚¹è®¡ç®—è¿åŠ¨é€Ÿåº¦
        key_indices = [0, 1, 2, 3, 4, 5, 6, 7]  # æ‰€æœ‰8ä¸ªå…³é”®ç‚¹
        total_movement = 0
        valid_points = 0
        
        for i in key_indices:
            if (i < len(current_keypoints) and i < len(prev_keypoints) and 
                current_keypoints[i][2] > 0.5 and prev_keypoints[i][2] > 0.5):
                movement = self.calculate_distance(current_keypoints[i][:2], prev_keypoints[i][:2])
                total_movement += movement
                valid_points += 1
        
        if valid_points > 0:
            avg_movement = total_movement / valid_points
            # å½’ä¸€åŒ–åˆ°-1åˆ°1ï¼Œå¿«é€Ÿè¿åŠ¨ä¸ºæ­£å€¼
            normalized_speed = min(avg_movement / 15.0, 1.0) * 2 - 1  # 15åƒç´ ä½œä¸ºåŸºå‡†
            return normalized_speed
        else:
            return 0
    
    def analyze_flow_consistency(self, current_keypoints):
        """åˆ†æåŠ¨ä½œæµç•…æ€§ - Flowç»´åº¦ï¼ˆåŸºäºè‚©è‚˜å…³é”®ç‚¹ï¼‰"""
        if len(self.keypoint_history) < 3:
            return 0
        
        # é‡ç‚¹åˆ†æè‚©è†€å’Œè‚˜éƒ¨çš„æµç•…æ€§
        key_indices = [0, 1, 2, 3]  # è‚©è†€å’Œè‚˜éƒ¨
        speed_changes = []
        
        for i in range(len(self.keypoint_history) - 1):
            curr_frame = self.keypoint_history[i]
            next_frame = self.keypoint_history[i + 1]
            
            frame_movement = 0
            valid_points = 0
            
            for j in key_indices:
                if (j < len(curr_frame) and j < len(next_frame) and
                    curr_frame[j][2] > 0.5 and next_frame[j][2] > 0.5):
                    movement = self.calculate_distance(curr_frame[j][:2], next_frame[j][:2])
                    frame_movement += movement
                    valid_points += 1
            
            if valid_points > 0:
                speed_changes.append(frame_movement / valid_points)
        
        if len(speed_changes) < 2:
            return 0
        
        # è®¡ç®—æµç•…æ€§ï¼ˆå˜åŒ–è¶Šå°è¶Šæµç•…ï¼‰
        mean_speed = sum(speed_changes) / len(speed_changes)
        if mean_speed == 0:
            return 1
            
        variance = sum((x - mean_speed) ** 2 for x in speed_changes) / len(speed_changes)
        relative_variance = variance / (mean_speed + 1e-6)
        
        # å½’ä¸€åŒ–åˆ°-1åˆ°1ï¼Œæµç•…ä¸ºæ­£å€¼ï¼Œä¸æµç•…ä¸ºè´Ÿå€¼
        flow_score = 1 / (1 + relative_variance)
        return flow_score * 2 - 1
    
    def analyze_space_directness(self, keypoints):
        """åˆ†æç©ºé—´ä½¿ç”¨çš„ç›´æ¥æ€§ - Spaceç»´åº¦ï¼ˆåŸºäº8ä¸ªå…³é”®ç‚¹ï¼‰"""
        if len(keypoints) < 8:
            return 0
        
        # è®¡ç®—èº«ä½“ä¸­å¿ƒï¼ˆè‚©è†€å’Œé«‹éƒ¨çš„ä¸­ç‚¹ï¼‰
        center_x = (keypoints[0][0] + keypoints[1][0] + keypoints[4][0] + keypoints[5][0]) / 4
        center_y = (keypoints[0][1] + keypoints[1][1] + keypoints[4][1] + keypoints[5][1]) / 4
        
        # åˆ†æå››è‚¢ç›¸å¯¹äºèº«ä½“ä¸­å¿ƒçš„ä½ç½®ï¼ˆä½¿ç”¨è‚˜éƒ¨å’Œè†éƒ¨ï¼‰
        limb_positions = []
        for idx in [2, 3, 6, 7]:  # è‚˜éƒ¨å’Œè†éƒ¨
            if keypoints[idx][2] > 0.5:
                distance = self.calculate_distance([center_x, center_y], keypoints[idx][:2])
                limb_positions.append(distance)
        
        if not limb_positions:
            return 0
        
        # è®¡ç®—åŠ¨ä½œèŒƒå›´ï¼ˆé—´æ¥æ€§æŒ‡æ ‡ï¼‰
        avg_distance = sum(limb_positions) / len(limb_positions)
        max_distance = max(limb_positions)
        
        # å¦‚æœåŠ¨ä½œèŒƒå›´å¤§ä¸”å˜åŒ–å¤§ï¼Œåˆ™æ›´é—´æ¥
        if avg_distance > 0:
            range_ratio = max_distance / avg_distance
            # å½’ä¸€åŒ–åˆ°-1åˆ°1ï¼Œå¤§èŒƒå›´åŠ¨ä½œä¸ºæ­£å€¼ï¼ˆé—´æ¥ï¼‰ï¼Œå°èŒƒå›´ä¸ºè´Ÿå€¼ï¼ˆç›´æ¥ï¼‰
            space_score = min((range_ratio - 1.0) / 2.0, 1.0)
            return max(-1, min(1, space_score))
        
        return 0
    
    def calculate_laban_qualities(self, keypoints):
        """è®¡ç®—æ‹‰ç­è¿åŠ¨è´¨é‡"""
        self.keypoint_history.append(keypoints)
        
        expansion = self.analyze_body_expansion(keypoints)
        vertical_pos = self.analyze_vertical_position(keypoints)
        movement_speed = self.analyze_movement_speed(keypoints)
        flow_consistency = self.analyze_flow_consistency(keypoints)
        space_directness = self.analyze_space_directness(keypoints)
        
        # Weight: åŸºäºèº«ä½“æ‰©å¼ åº¦ï¼ˆæ‰©å¼ ä¸ºå¼ºï¼Œæ”¶ç¼©ä¸ºè½»ï¼‰
        self.effort_qualities['weight'] = float(expansion * 0.7 + vertical_pos * 0.3)
        
        # Time: ç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–åçš„è¿åŠ¨é€Ÿåº¦
        self.effort_qualities['time'] = float(movement_speed)
        
        # Flow: ç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–åçš„æµç•…æ€§
        self.effort_qualities['flow'] = float(flow_consistency)
        
        # Space: ç›´æ¥ä½¿ç”¨æ ‡å‡†åŒ–åçš„ç©ºé—´ç›´æ¥æ€§
        self.effort_qualities['space'] = float(space_directness)
        
        # ç¡®ä¿å€¼åœ¨-1åˆ°1èŒƒå›´å†…
        for key in self.effort_qualities:
            self.effort_qualities[key] = max(-1, min(1, self.effort_qualities[key]))
        
        return {k: float(v) for k, v in self.effort_qualities.items()}
    
    def recognize_emotion(self, laban_qualities):
        """åŸºäºæ‹‰ç­è´¨é‡è¯†åˆ«æƒ…æ„Ÿ"""
        emotion_scores = {}
        
        # è®¡ç®—ä¸æ¯ç§æƒ…æ„Ÿçš„ç›¸ä¼¼åº¦
        for emotion, template in self.emotion_map.items():
            # ä½¿ç”¨åŠ æƒæ¬§æ°è·ç¦»
            weights = {'weight': 1.2, 'time': 1.0, 'flow': 1.1, 'space': 0.9}
            distance = 0
            for quality in ['weight', 'time', 'flow', 'space']:
                diff = laban_qualities[quality] - template[quality]
                distance += weights[quality] * (diff ** 2)
            
            distance = math.sqrt(distance)
            # ä½¿ç”¨æ›´æ¸©å’Œçš„ç›¸ä¼¼åº¦å‡½æ•°
            similarity = math.exp(-distance / 0.8)  # 0.8æ˜¯æ¸©åº¦å‚æ•°
            emotion_scores[emotion] = float(similarity)
        
        # æ‰¾åˆ°æœ€é«˜å¾—åˆ†çš„æƒ…æ„Ÿ
        if emotion_scores:
            best_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
            best_score = emotion_scores[best_emotion]
            
            # å¦‚æœæœ€é«˜å¾—åˆ†å¤ªä½ï¼Œè®¤ä¸ºæ˜¯ä¸­æ€§çŠ¶æ€
            if best_score < 0.3:
                recognized_emotion = 'ä¸­æ€§/è‡ªç„¶'
            else:
                recognized_emotion = best_emotion
        else:
            recognized_emotion = 'ä¸­æ€§/è‡ªç„¶'
        
        return recognized_emotion, emotion_scores

def ensure_json_serializable(obj):
    """ç¡®ä¿å¯¹è±¡å¯ä»¥JSONåºåˆ—åŒ–"""
    if isinstance(obj, dict):
        return {k: ensure_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [ensure_json_serializable(item) for item in obj]
    elif isinstance(obj, (np.integer, np.floating, np.ndarray)):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj.item()
    elif isinstance(obj, (int, float, str, bool, type(None))):
        return obj
    else:
        return str(obj)

class PoseAnalysisTool(BaseTool):
    """LangChainå§¿æ€åˆ†æå·¥å…·"""
    
    name: str = "pose_analysis"
    description: str = """
    å§¿æ€åˆ†æå·¥å…·ï¼Œæ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
    1. å®æ—¶åˆ†æï¼šä»æ‘„åƒå¤´å®æ—¶åˆ†æå§¿æ€å’Œæƒ…æ„Ÿï¼ˆåŸºäºæ‹‰ç­è¿åŠ¨ç†è®ºï¼‰
    2. è·å–æ±‡æ€»ï¼šè·å–åˆ†æç»“æœæ±‡æ€»
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - {"action": "analyze_realtime", "duration": 10} - å®æ—¶åˆ†æ10ç§’
    - {"action": "get_summary"} - è·å–åˆ†ææ±‡æ€»
    """
    args_schema: Type[BaseModel] = PoseAnalysisInput
    # ç§»é™¤ return_directï¼Œè®©Agentèƒ½å¤Ÿå¤„ç†å·¥å…·è¾“å‡º
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶ï¼Œé¿å…åœ¨å·¥å…·åˆ›å»ºæ—¶å°±åŠ è½½æ‰€æœ‰ä¾èµ–
        self._model = None
        self._analyzer = None
        self._analysis_results = []
    
    def _detect_available_cameras(self):
        """æ£€æµ‹å¯ç”¨æ‘„åƒå¤´ï¼ˆæ”¹è¿›ç‰ˆï¼Œåªæ£€æµ‹æ‘„åƒå¤´0ï¼‰"""
        available_cameras = []
        
        # é¦–å…ˆå°è¯•é‡Šæ”¾å¯èƒ½è¢«å ç”¨çš„æ‘„åƒå¤´èµ„æº
        import gc
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        
        cap = None
        try:
            print("æ£€æµ‹æ‘„åƒå¤´ 0...")
            cap = cv2.VideoCapture(0)
            
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            
            if cap.isOpened():
                # å°è¯•è¯»å–ä¸€å¸§æ¥ç¡®è®¤æ‘„åƒå¤´çœŸæ­£å¯ç”¨
                ret, frame = cap.read()
                if ret and frame is not None and frame.size > 0:
                    available_cameras.append(0)
                    print("âœ… å‘ç°å¯ç”¨æ‘„åƒå¤´: 0")
                else:
                    print("âŒ æ‘„åƒå¤´ 0 æ‰“å¼€ä½†æ— æ³•è¯»å–æ•°æ®")
            else:
                print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ 0")
                
        except Exception as e:
            print(f"âŒ æ£€æµ‹æ‘„åƒå¤´ 0 æ—¶å‡ºé”™: {e}")
        finally:
            # ç¡®ä¿é‡Šæ”¾èµ„æº
            if cap is not None:
                cap.release()
                
        # å†æ¬¡åƒåœ¾å›æ”¶
        gc.collect()
        
        if available_cameras:
            print("âœ… æ‘„åƒå¤´ 0 å¯ç”¨")
        else:
            print("âš ï¸ æ‘„åƒå¤´ 0 ä¸å¯ç”¨")
            
        return available_cameras
    
    def _get_analyzer(self):
        """è·å–æ‹‰ç­è¿åŠ¨åˆ†æå™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._analyzer is None:
            self._analyzer = LabanMovementAnalyzer()
            logger.info("æ‹‰ç­è¿åŠ¨åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        return self._analyzer
    
    def _load_model(self, model_path: str) -> bool:
        """åŠ è½½YOLOæ¨¡å‹"""
        if not YOLO_AVAILABLE:
            logger.error("YOLOæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
            return False
        
        try:
            if not os.path.exists(model_path):
                logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}ï¼Œå°è¯•ä¸‹è½½é»˜è®¤æ¨¡å‹")
                model_path = "yolov8n-pose.pt"
            
            self._model = YOLO(model_path)
            logger.info(f"YOLOæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return True
        except Exception as e:
            logger.error(f"YOLOæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _detect_pose(self, frame, confidence_threshold: float = 0.5) -> Optional[List]:
        """æ£€æµ‹å§¿æ€å…³é”®ç‚¹å¹¶è½¬æ¢ä¸ºç®€åŒ–çš„8ä¸ªå…³é”®ç‚¹"""
        if not self._model:
            return None
        
        try:
            results = self._model(frame, verbose=False, conf=confidence_threshold, max_det=1)
            
            if results[0].keypoints is not None and len(results[0].keypoints) > 0:
                yolo_keypoints = results[0].keypoints.data[0].cpu().numpy()
                
                # è½¬æ¢ä¸ºç®€åŒ–çš„8ä¸ªå…³é”®ç‚¹
                simplified_keypoints = []
                for simplified_idx, yolo_idx in YOLO_TO_SIMPLIFIED_MAPPING.items():
                    if yolo_idx < len(yolo_keypoints):
                        kp = yolo_keypoints[yolo_idx]
                        # å°†numpy float32è½¬æ¢ä¸ºPython float
                        simplified_keypoints.append([float(kp[0]), float(kp[1]), float(kp[2])])
                    else:
                        # å¦‚æœYOLOå…³é”®ç‚¹ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        simplified_keypoints.append([0.0, 0.0, 0.0])
                
                return simplified_keypoints
            else:
                return None
        except Exception as e:
            logger.error(f"å§¿æ€æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    def _draw_pose_keypoints(self, frame, keypoints: List, min_confidence: float = 0.5):
        """åœ¨å¸§ä¸Šç»˜åˆ¶ç®€åŒ–çš„8ä¸ªå§¿æ€å…³é”®ç‚¹"""
        if not keypoints or len(keypoints) < 8:
            return frame
        
        # å®šä¹‰ç®€åŒ–çš„å…³é”®ç‚¹è¿æ¥ï¼ˆåŸºäº8ä¸ªå…³é”®ç‚¹ï¼‰
        # ç´¢å¼•ï¼š0=å·¦è‚©, 1=å³è‚©, 2=å·¦è‚˜, 3=å³è‚˜, 4=å·¦é«‹, 5=å³é«‹, 6=å·¦è†, 7=å³è†
        pose_connections = [
            (0, 2),   # å·¦è‚©-å·¦è‚˜
            (1, 3),   # å³è‚©-å³è‚˜
            (0, 1),   # å·¦è‚©-å³è‚©
            (4, 6),   # å·¦é«‹-å·¦è†
            (5, 7),   # å³é«‹-å³è†
            (4, 5),   # å·¦é«‹-å³é«‹
            (0, 4),   # å·¦è‚©-å·¦é«‹
            (1, 5),   # å³è‚©-å³é«‹
        ]
        
        # ç»˜åˆ¶å…³é”®ç‚¹è¿æ¥çº¿
        for connection in pose_connections:
            pt1_idx, pt2_idx = connection
            if pt1_idx < len(keypoints) and pt2_idx < len(keypoints):
                x1, y1, conf1 = keypoints[pt1_idx]
                x2, y2, conf2 = keypoints[pt2_idx]
                
                if conf1 > min_confidence and conf2 > min_confidence:
                    cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
        
        # å…³é”®ç‚¹åç§°å’Œé¢œè‰²æ˜ å°„
        keypoint_colors = {
            0: (255, 0, 0),    # å·¦è‚© - è“è‰²
            1: (255, 0, 0),    # å³è‚© - è“è‰²
            2: (0, 255, 0),    # å·¦è‚˜ - ç»¿è‰²
            3: (0, 255, 0),    # å³è‚˜ - ç»¿è‰²
            4: (0, 0, 255),    # å·¦é«‹ - çº¢è‰²
            5: (0, 0, 255),    # å³é«‹ - çº¢è‰²
            6: (255, 255, 0),  # å·¦è† - é’è‰²
            7: (255, 255, 0),  # å³è† - é’è‰²
        }
        
        # ç»˜åˆ¶å…³é”®ç‚¹
        for i, (x, y, confidence) in enumerate(keypoints):
            if confidence > min_confidence and i < len(KEYPOINT_NAMES):
                color = keypoint_colors.get(i, (128, 128, 128))
                
                cv2.circle(frame, (int(x), int(y)), 6, color, -1)
                # æ·»åŠ å…³é”®ç‚¹åç§°
                cv2.putText(frame, KEYPOINT_NAMES[i], (int(x) + 8, int(y) - 8), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1, cv2.LINE_AA)
        
        return frame
    
    def _analyze_realtime(self, duration: int, confidence_threshold: float, 
                         save_frames: bool = True, save_interval: int = 30) -> Dict[str, Any]:
        """å®æ—¶å§¿æ€åˆ†æ - æ”¹è¿›ç‰ˆï¼Œå¤„ç†æ‘„åƒå¤´èµ„æºå†²çª"""
        import time
        import gc
        
        print(f"ğŸ¯ å¼€å§‹{duration}ç§’å®æ—¶å§¿æ€åˆ†æ...")
        
        # å¼ºåˆ¶é‡Šæ”¾å¯èƒ½å ç”¨çš„æ‘„åƒå¤´èµ„æº
        print("â³ ç­‰å¾…æ‘„åƒå¤´èµ„æºå®Œå…¨é‡Šæ”¾...")
        gc.collect()  # åƒåœ¾å›æ”¶
        cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰OpenCVçª—å£
        time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿èµ„æºé‡Šæ”¾
        
        # æ£€æµ‹å¯ç”¨æ‘„åƒå¤´
        available_cameras = self._detect_available_cameras()
        
        if not available_cameras:
            # å¦‚æœæ²¡æœ‰å¯ç”¨æ‘„åƒå¤´ï¼Œå°è¯•ç­‰å¾…å¹¶é‡æ–°æ£€æµ‹
            print("âš ï¸ é¦–æ¬¡æ£€æµ‹æœªå‘ç°æ‘„åƒå¤´ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
            time.sleep(5)
            available_cameras = self._detect_available_cameras()
            
            if not available_cameras:
                return {
                    'success': False,
                    'message': 'æœªå‘ç°å¯ç”¨æ‘„åƒå¤´ï¼Œå¯èƒ½è¢«å…¶ä»–ç¨‹åºå ç”¨ã€‚è¯·ç¡®ä¿å›¾åƒåˆ†æå·²å®Œæˆã€‚',
                    'data': None
                }
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ‘„åƒå¤´
        camera_id = available_cameras[0]
        print(f"ğŸ“¹ ä½¿ç”¨æ‘„åƒå¤´ {camera_id} è¿›è¡Œå®æ—¶å§¿æ€åˆ†æ")
        
        cap = None
        try:
            cap = cv2.VideoCapture(camera_id)
            if not cap.isOpened():
                # å°è¯•é‡æ–°æ‰“å¼€
                print("ğŸ”„ é‡æ–°å°è¯•æ‰“å¼€æ‘„åƒå¤´...")
                time.sleep(2)
                cap.release()
                cap = cv2.VideoCapture(camera_id)
                
                if not cap.isOpened():
                    return {
                        'success': False,
                        'message': f'æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}ï¼Œå¯èƒ½è¢«å ç”¨',
                        'data': None
                    }
            
            # è®¾ç½®åˆ†è¾¨ç‡å’Œç¼“å†²åŒº
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒº
            
            # ç­‰å¾…æ‘„åƒå¤´å®Œå…¨åˆå§‹åŒ–
            time.sleep(1)
            
            analyzer = self._get_analyzer()
            analysis_results = []
            start_time = time.time()
            frame_count = 0
            consecutive_failures = 0
            
            print("ğŸ¬ å¼€å§‹å®æ—¶å§¿æ€åˆ†æ...")
            
            while (time.time() - start_time) < duration:
                ret, frame = cap.read()
                if not ret or frame is None:
                    consecutive_failures += 1
                    if consecutive_failures > 10:
                        print("âŒ è¿ç»­è¯»å–å¤±è´¥è¿‡å¤šï¼Œå¯èƒ½æ‘„åƒå¤´è¢«æ–­å¼€")
                        break
                    print("â­ï¸ è·³è¿‡æ— æ•ˆå¸§")
                    time.sleep(0.1)
                    continue
                
                consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                frame_count += 1
                
                # æ£€æµ‹å§¿æ€
                keypoints = self._detect_pose(frame, confidence_threshold)
                if keypoints is None:
                    if frame_count % 10 == 0:  # æ¯10å¸§æ‰“å°ä¸€æ¬¡
                        print(f"ğŸ“Š å¸§ {frame_count}: æœªæ£€æµ‹åˆ°å§¿æ€")
                    continue
                
                visible_keypoints = len([kp for kp in keypoints if kp[2] > 0.5])
                print(f"âœ… å¸§ {frame_count}: æ£€æµ‹åˆ° {visible_keypoints}/8 ä¸ªå…³é”®ç‚¹")
                
                # æ‹‰ç­è¿åŠ¨åˆ†æ
                laban_qualities = analyzer.calculate_laban_qualities(keypoints)
                emotion, emotion_scores = analyzer.recognize_emotion(laban_qualities)
                
                result = {
                    'timestamp': datetime.now().isoformat(),
                    'frame_count': frame_count,
                    'laban_qualities': laban_qualities,
                    'recognized_emotion': emotion,
                    'emotion_scores': emotion_scores,
                    'visible_keypoints': visible_keypoints
                }
                
                # ä¿å­˜å…³é”®å¸§åˆ°æœ¬åœ°å­˜å‚¨
                if save_frames and frame_count % save_interval == 0:
                    try:
                        # åœ¨å¸§ä¸Šç»˜åˆ¶å§¿æ€å…³é”®ç‚¹
                        annotated_frame = self._draw_pose_keypoints(frame.copy(), keypoints)
                        
                        # ç”Ÿæˆæ–‡ä»¶å
                        filename = storage_manager.generate_filename(
                            f"pose_frame_{frame_count}_{emotion}", ".jpg"
                        )
                        
                        # ä¿å­˜å¸¦å§¿æ€æ ‡æ³¨çš„å¸§
                        saved_path = storage_manager.save_image_from_frame(
                            annotated_frame, filename, "pose_analysis"
                        )
                        
                        if saved_path:
                            result['saved_frame_path'] = saved_path
                            print(f"ğŸ“ å¸§ {frame_count} å·²ä¿å­˜åˆ°: {saved_path}")
                        else:
                            print(f"âš ï¸ å¸§ {frame_count} ä¿å­˜å¤±è´¥")
                            
                    except Exception as e:
                        print(f"âŒ ä¿å­˜å¸§ {frame_count} æ—¶å‡ºé”™: {e}")
                        logger.error(f"ä¿å­˜å§¿æ€å¸§å¤±è´¥: {e}")
                
                analysis_results.append(result)
                print(f"ğŸ’­ å¸§ {frame_count}: æ£€æµ‹åˆ°æƒ…æ„Ÿ -> {emotion}")
                
                # çŸ­æš‚ä¼‘çœ é¿å…CPUè¿‡è½½
                time.sleep(0.1)
            
            # ç»Ÿè®¡åˆ†æ
            if not analysis_results:
                return {
                    'success': False,
                    'message': 'åˆ†ææœŸé—´æœªæ£€æµ‹åˆ°æœ‰æ•ˆå§¿æ€æ•°æ®',
                    'data': None
                }
            
            emotions = [r['recognized_emotion'] for r in analysis_results]
            emotion_counts = {}
            for emotion in emotions:
                emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
            
            dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else 'æœªçŸ¥'
            
            # è®¡ç®—å¹³å‡æ‹‰ç­è´¨é‡
            avg_laban = {}
            for quality in ['weight', 'time', 'flow', 'space']:
                avg_laban[quality] = float(sum(r['laban_qualities'][quality] for r in analysis_results) / len(analysis_results))
            
            # ç»Ÿè®¡ä¿å­˜çš„å¸§æ•°
            saved_frames = [r for r in analysis_results if 'saved_frame_path' in r]
            saved_frame_paths = [r['saved_frame_path'] for r in saved_frames]
            
            summary = {
                'duration_seconds': int(time.time() - start_time),
                'total_frames': frame_count,
                'valid_analyses': len(analysis_results),
                'dominant_emotion': dominant_emotion,
                'emotion_distribution': emotion_counts,
                'average_laban_qualities': avg_laban,
                'latest_results': analysis_results[-5:],  # åªä¿ç•™æœ€å5ä¸ªç»“æœ
                'saved_frames_count': len(saved_frames),
                'saved_frame_paths': saved_frame_paths,
                'frame_saving_enabled': save_frames,
                'save_interval': save_interval
            }
            
            self._analysis_results.append(summary)
            
            print(f"ğŸ‰ å§¿æ€åˆ†æå®Œæˆï¼å¤„ç†{frame_count}å¸§ï¼Œæœ‰æ•ˆåˆ†æ{len(analysis_results)}æ¬¡")
            print(f"ğŸ­ ä¸»å¯¼æƒ…æ„Ÿ: {dominant_emotion}")
            
            return {
                'success': True,
                'message': f'å®æ—¶åˆ†æå®Œæˆï¼Œå¤„ç†{frame_count}å¸§ï¼Œæœ‰æ•ˆåˆ†æ{len(analysis_results)}æ¬¡',
                'data': summary
            }
            
        except Exception as e:
            logger.error(f"å®æ—¶åˆ†æå¼‚å¸¸: {e}")
            return {
                'success': False,
                'message': f'å®æ—¶åˆ†æå¼‚å¸¸: {str(e)}',
                'data': None
            }
        finally:
            # ç¡®ä¿é‡Šæ”¾æ‘„åƒå¤´èµ„æº
            if cap is not None:
                cap.release()
                print("ğŸ“¹ å§¿æ€åˆ†ææ‘„åƒå¤´å·²é‡Šæ”¾")
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰OpenCVçª—å£
    
    def _get_analysis_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ±‡æ€»"""
        # æ£€æµ‹æ‘„åƒå¤´å¯ç”¨æ€§
        available_cameras = self._detect_available_cameras()
        
        return {
            'total_analyses': len(self._analysis_results),
            'yolo_available': YOLO_AVAILABLE,
            'model_loaded': self._model is not None,
            'available_cameras': available_cameras,
            'camera_count': len(available_cameras),
            'analyzer_initialized': self._analyzer is not None,
            'keypoint_names': KEYPOINT_NAMES,
            'emotion_categories': list(LabanMovementAnalyzer().emotion_map.keys())
        }
    
    def _format_laban_analysis_dict(self, analysis_data: Dict) -> Dict[str, Any]:
        """å°†åˆ†ææ•°æ®æ ¼å¼åŒ–ä¸ºæ‹‰ç­åˆ†æå­—å…¸"""
        if not analysis_data or 'average_laban_qualities' not in analysis_data:
            return None
            
        laban = analysis_data['average_laban_qualities']
        return {
            'space': {
                'direction': 'direct' if laban.get('space', 0) > 0 else 'indirect',
                'path': 'straight' if laban.get('space', 0) > 0.5 else 'curved',
                'range': 'wide' if abs(laban.get('space', 0)) > 0.5 else 'narrow'
            },
            'time': {
                'speed': 'fast' if laban.get('time', 0) > 0 else 'slow',
                'rhythm': 'regular' if abs(laban.get('time', 0)) < 0.5 else 'irregular',
                'duration': 'sustained' if laban.get('time', 0) < 0 else 'quick'
            },
            'weight': {
                'strength': 'strong' if laban.get('weight', 0) > 0 else 'light',
                'weight': 'heavy' if laban.get('weight', 0) > 0.5 else 'light',
                'energy': 'powerful' if laban.get('weight', 0) > 0 else 'gentle'
            },
            'flow': {
                'control': 'controlled' if laban.get('flow', 0) < 0 else 'free',
                'continuity': 'bound' if laban.get('flow', 0) < 0 else 'flowing',
                'tension': 'tense' if laban.get('flow', 0) < -0.5 else 'relaxed'
            },
            'movement_quality': analysis_data.get('dominant_emotion', 'æœªçŸ¥'),
            'raw_qualities': laban
        }

    def _run(
        self,
        action: str,
        model_path: str = "yolov8n-pose.pt",
        confidence_threshold: float = 0.5,
        duration: int = 10,
        save_frames: bool = True,
        save_interval: int = 30,
        run_manager: Optional[CallbackManagerForToolRun] = None,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·æ“ä½œ"""
        try:
            logger.info(f"æ‰§è¡Œå§¿æ€åˆ†ææ“ä½œ: {action}")
            
            if action == "analyze_realtime":
                # æ£€æŸ¥YOLOå¯ç”¨æ€§
                if not YOLO_AVAILABLE:
                    return {
                        'success': False,
                        'message': 'YOLOæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ultralytics',
                        'data': None,
                        'laban_analysis': None
                    }
                
                # åŠ è½½æ¨¡å‹
                if not self._load_model(model_path):
                    return {
                        'success': False,
                        'message': f'æ— æ³•åŠ è½½YOLOæ¨¡å‹: {model_path}',
                        'data': None,
                        'laban_analysis': None
                    }
                
                result = self._analyze_realtime(duration, confidence_threshold, save_frames, save_interval)
                
                # å¦‚æœåˆ†ææˆåŠŸï¼Œæ·»åŠ æ‹‰ç­åˆ†ææ•°æ®åˆ°é¡¶å±‚
                if result['success'] and result['data']:
                    result['laban_analysis'] = self._format_laban_analysis_dict(result['data'])
                
                return result
                
            elif action == "get_summary":
                summary = self._get_analysis_summary()
                return {
                    'success': True,
                    'message': 'è·å–åˆ†ææ±‡æ€»æˆåŠŸ',
                    'data': summary,
                    'laban_analysis': None
                }
                
            else:
                return {
                    'success': False,
                    'message': f'ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {action}',
                    'data': None,
                    'laban_analysis': None
                }
                
        except Exception as e:
            logger.error(f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}")
            return {
                'success': False,
                'message': f'å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}',
                'data': None,
                'laban_analysis': None
            }

def main():
    """ç‹¬ç«‹è¿è¡Œæµ‹è¯•å‡½æ•°"""
    print("ğŸ¤– LETDANCE å§¿æ€åˆ†æå·¥å…· - ç®€åŒ–ç‰ˆï¼ˆ8ä¸ªå…³é”®ç‚¹ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    tool = PoseAnalysisTool()
    
    # æ˜¾ç¤ºå·¥å…·ä¿¡æ¯
    print(f"å·¥å…·åç§°: {tool.name}")
    print(f"å·¥å…·æè¿°: {tool.description}")
    print(f"YOLOå¯ç”¨æ€§: {YOLO_AVAILABLE}")
    print(f"å…³é”®ç‚¹æ•°é‡: {len(KEYPOINT_NAMES)}")
    print(f"å…³é”®ç‚¹åˆ—è¡¨: {', '.join(KEYPOINT_NAMES)}")
    
    # æµ‹è¯•è·å–æ±‡æ€»
    print("\nğŸ“Š è·å–åˆ†ææ±‡æ€»...")
    summary_result = tool._run(action="get_summary")
    print(f"æ±‡æ€»ç»“æœ: {summary_result}")
    
    # å¦‚æœYOLOå¯ç”¨ï¼Œæä¾›å®æ—¶åˆ†æé€‰é¡¹
    if YOLO_AVAILABLE:
        print("\nğŸ¯ æ˜¯å¦è¿›è¡Œå®æ—¶å§¿æ€åˆ†æï¼Ÿ")
        choice = input("è¾“å…¥ y è¿›è¡Œ5ç§’å®æ—¶åˆ†æï¼Œå…¶ä»–é”®è·³è¿‡: ").strip().lower()
        
        if choice == 'y':
            print("å¼€å§‹5ç§’å®æ—¶å§¿æ€åˆ†æ...")
            print("ğŸ’¡ æ³¨æ„ï¼šç°åœ¨åªä½¿ç”¨8ä¸ªå…³é”®ç‚¹è¿›è¡Œæ‹‰ç­è¿åŠ¨åˆ†æ")
            analysis_result = tool._run(
                action="analyze_realtime",
                duration=5,
                confidence_threshold=0.5
            )
            print(f"åˆ†æç»“æœ: {analysis_result}")
    else:
        print("\nâš ï¸  YOLOæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå®æ—¶åˆ†æ")
        print("å®‰è£…å‘½ä»¤: pip install ultralytics")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()